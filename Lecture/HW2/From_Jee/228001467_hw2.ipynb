{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Fall 2019\n",
    "\n",
    "\n",
    "# Homework 2\n",
    "\n",
    "- **100 points [10% of your final grade]**\n",
    "- **Due Saturday, October 19 by 11:59pm**\n",
    "\n",
    "**Goals of this homework:** There are five objectives of this homework: \n",
    "\n",
    "* Become familiar with Apache Spark and working in a distributed environment in the cloud\n",
    "* Get hands-on experience designing and running a simple MapReduce data transformation job\n",
    "* Get hands-on experience using Spark built-in functions; namely, LDA and PageRank\n",
    "* Design a Pregel algorithm to find tree depth in a network\n",
    "* Understand and implement Trawling algorithm to find user communities\n",
    "\n",
    "*Submission instructions:* You should post your notebook to ecampus (look for the homework 2 assignment there). Name your submission **your-uin_hw2.ipynb**, so for example, my submission would be something like **555001234_hw2.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that. Follow the AWS guide to create a Hadoop/Spark cluster and create an empty Notebook. Copy all the cells in this notebook to the AWS notebook and continue working on your notebook in AWS. When you are done, download your notebook from AWS (navigate to the location on S3 where your notebook is saved and click download) and submit it to ecampus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Dataset\n",
    "We will use a dataset of tweets concerning members of the US congress. The data spans almost a year (from October 3rd, 2018 to September 25th, 2019) covering 577 of the members. Any tweet or retweet posted by the 577 members or directed to them by other Twitter users were collected.\n",
    "\n",
    "The data is on S3 in a bucket named s3://us-congress-tweets that you can access. There are 277,744,063 tweets. This is a huge dataset so we will not be working directly on this data all the time. Rather we will work on samples or subsets of this data but in some cases, we will ask you to execute your task on the whole dataset.\n",
    "\n",
    "Below is a summary of all datasets used for this homework:\n",
    "\n",
    "| Dataset                | Location in S3                                      | Description |\n",
    "| :---                   | :---                                                | :---\n",
    "| Congress members       | s3://us-congress-tweets/congress_members.csv        | 577 twitter ids and screen names |\n",
    "| Raw tweets             | s3://us-congress-tweets/raw/\\*.snappy               | the whole json objects of the tweets|\n",
    "| Sample tweets          | s3://us-congress-tweets/congress-sample-10k.json.gz | 10k sample tweets|\n",
    "| Trimmed tweets         | s3://us-congress-tweets/trimmed/\\*.parquet          | selected fields for all tweets|\n",
    "| User hashtags          | s3://us-congress-tweets/user_hashtags.csv           | all pairs of <user, hashtag>|\n",
    "| User replies           | s3://us-congress-tweets/reply_network.csv           | all pairs of <reply_tweet, replied_to_tweet> |\n",
    "| User mentions           | s3://us-congress-tweets/user_mentions.csv           | all pairs of <src_user_id, src_dest_id, frequency> |\n",
    "\n",
    "Let's run some exploration below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|            userid|    screen_name|\n",
      "+------------------+---------------+\n",
      "|         776664410|  RepCartwright|\n",
      "|         240363117|   RepTomMarino|\n",
      "|837722935095930883| RepScottTaylor|\n",
      "|        1069124515|     RepLaMalfa|\n",
      "|818460870573441028|  RepTomGarrett|\n",
      "|         163570705|     repcleaver|\n",
      "|          19739126|      GOPLeader|\n",
      "|          33563161| RepJoseSerrano|\n",
      "|        2861616083|USRepGaryPalmer|\n",
      "|        1074518754| SenatorBaldwin|\n",
      "|         305620929|  Call_Me_Dutch|\n",
      "|         381152398| RepTerriSewell|\n",
      "|         834069080| RepDavidRouzer|\n",
      "|         249787913|  SenatorCarper|\n",
      "|         188019606|        Clyburn|\n",
      "|         217543151|SenatorTimScott|\n",
      "|          39249305| USRepMikeDoyle|\n",
      "|          33537967|   amyklobuchar|\n",
      "|         249410485|  SanfordBishop|\n",
      "|          23124635|    TomColeOK04|\n",
      "+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "('Number of congress members tracked:', 577)\n"
     ]
    }
   ],
   "source": [
    "# First let's read Twitter ids and screen names of the 577 US congress members\n",
    "\n",
    "congress_members = spark.read.csv(\"s3://us-congress-tweets/congress_members.csv\", header=True)\n",
    "congress_members.show()\n",
    "print(\"Number of congress members tracked:\", congress_members.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `spark.read.json(...)` without schema to load the tweets into a dataframe but this will be slow for two reasons:\n",
    "* First, it will make one pass over the data to build a schema of the content, then a second pass to read the content and parse it to the dataframe. \n",
    "* It will read all the content of the Tweet JSON objects but we only need few fields for a given task.\n",
    "\n",
    "Thus we define our own schema something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "twitter_date_format=\"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField('created_at',TimestampType(),True),\n",
    "    StructField('followers_count',LongType(),True),\n",
    "    StructField('id',LongType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('screen_name',StringType(),True)\n",
    "])\n",
    "\n",
    "hashtag_schema = ArrayType(StructType([StructField('text',StringType(),True)]))\n",
    "user_mentions_schema = ArrayType(StructType([StructField('id',LongType(),True),\n",
    "                                             StructField('screen_name',StringType(),True)]))\n",
    "entities_schema = StructType([\n",
    "    StructField('hashtags',hashtag_schema,True),\n",
    "    StructField('user_mentions',user_mentions_schema,True)\n",
    "    ])\n",
    "\n",
    "retweeted_status_schema =StructType([        \n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_user_id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_status_id\", LongType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"user\", user_schema)\n",
    "    ])\n",
    "\n",
    "tweet_schema =StructType([\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_user_id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_status_id\", LongType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"user\", user_schema),\n",
    "        StructField(\"entities\", entities_schema),\n",
    "        StructField(\"retweeted_status\", retweeted_status_schema)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to read the tweets with `spark.read.json` passing our own schema as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- in_reply_to_user_id: long (nullable = true)\n",
      " |-- in_reply_to_status_id: long (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- in_reply_to_user_id: long (nullable = true)\n",
      " |    |-- in_reply_to_status_id: long (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = spark.read.option(\"timestampFormat\", twitter_date_format)\\\n",
    "                   .json('s3://us-congress-tweets/congress-sample-10k.json.gz', tweet_schema)\\\n",
    "                   .withColumn('user_id',F.col('user.id'))\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6 points) Part 1a: Exploratory Data Analysis (Small Scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique users and original tweets (i.e. not retweets) are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Unique user:', 9735)\n"
     ]
    }
   ],
   "source": [
    "# your code here for unique users\n",
    "users = tweets.select(F.col(\"user.id\"))\n",
    "users = users.dropDuplicates()\n",
    "print(\"Unique user:\", users.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Original tweets:', 3327)\n"
     ]
    }
   ],
   "source": [
    "# your code here for original tweets\n",
    "originals = tweets.select(F.col(\"retweeted_status.id\"))\n",
    "print(\"Original tweets:\", originals.filter(\"id is null\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are the ten most mentioned users in the sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|count|\n",
      "+--------------------+-----+\n",
      "|         Adam Schiff|  333|\n",
      "|         Marco Rubio|  250|\n",
      "|       Chuck Schumer|  210|\n",
      "|        Nancy Pelosi|  210|\n",
      "|        Chris Murphy|  148|\n",
      "|     Rep. Matt Gaetz|  124|\n",
      "|   Senator Rand Paul|   98|\n",
      "|Senator Jeff Merkley|   90|\n",
      "|       Amy Klobuchar|   84|\n",
      "|       Kamala Harris|   74|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code and output here\n",
    "mentioned = tweets.select(F.col(\"retweeted_status.user.name\")).filter(\"name is not null\")\n",
    "mentioned.groupBy(\"name\").count().sort(\"count\", ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top hashtags used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|            col|count|\n",
      "+---------------+-----+\n",
      "|    [Venezuela]|  102|\n",
      "|[TrumpShutdown]|   42|\n",
      "| [MaduroRegime]|   29|\n",
      "|       [Maduro]|   20|\n",
      "|         [MAGA]|   20|\n",
      "|  [NancyPelosi]|   19|\n",
      "|[MuellerReport]|   17|\n",
      "| [ForThePeople]|   14|\n",
      "|    [Kavanaugh]|   14|\n",
      "| [BuildTheWall]|   14|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code and output here\n",
    "tweets.select(F.explode(F.col(\"entities.hashtags\"))).groupBy(\"col\").count().sort(\"count\", ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4 points) Part 1b: Exploratory Data Analysis (Large Scale)\n",
    "Repeat the above queries but now against the whole dataset defined in the dataframe below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- in_reply_to_user_id: long (nullable = true)\n",
      " |-- in_reply_to_status_id: long (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- in_reply_to_user_id: long (nullable = true)\n",
      " |    |-- in_reply_to_status_id: long (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_files = [x[0] for x in spark.read.csv(\"s3://us-congress-tweets/trimmed/files.txt\").collect()]\n",
    "tweets_all = spark.read.parquet(*trimmed_files)\n",
    "tweets_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Unique user:', 10749403)\n"
     ]
    }
   ],
   "source": [
    "# your code here for unique users\n",
    "users_all = tweets_all.select(F.col(\"user.id\"))\n",
    "users_all = users_all.dropDuplicates()\n",
    "print(\"Unique user:\", users_all.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Original tweets:', 96887299)\n"
     ]
    }
   ],
   "source": [
    "# your code here for original tweets\n",
    "originals_all = tweets_all.select(F.col(\"retweeted_status.id\"))\n",
    "print(\"Original tweets:\", originals_all.filter(\"id is null\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|              name|  count|\n",
      "+------------------+-------+\n",
      "|       Adam Schiff|5279430|\n",
      "|   Rep. Jim Jordan|3549600|\n",
      "|     Chuck Schumer|3441048|\n",
      "|      Nancy Pelosi|3231205|\n",
      "|       Marco Rubio|3190516|\n",
      "|    Bernie Sanders|2786791|\n",
      "|Rep. Eric Swalwell|2767102|\n",
      "|    Lindsey Graham|2579218|\n",
      "|      Mark Meadows|2295392|\n",
      "|      Chris Murphy|2170298|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top mentioned users code and output here\n",
    "mentioned_all = tweets_all.select(F.col(\"retweeted_status.user.name\")).filter(\"name is not null\")\n",
    "mentioned_all.groupBy(\"name\").count().sort(\"count\", ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|            col|  count|\n",
      "+---------------+-------+\n",
      "|    [Venezuela]|1206418|\n",
      "|  [MoscowMitch]|1105708|\n",
      "|[TrumpShutdown]| 632069|\n",
      "|         [MAGA]| 469507|\n",
      "|[MuellerReport]| 405471|\n",
      "|  [NancyPelosi]| 359063|\n",
      "| [MaduroRegime]| 349757|\n",
      "|        [Trump]| 312651|\n",
      "| [BuildTheWall]| 311186|\n",
      "| [GreenNewDeal]| 272289|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top hashtags code and output here\n",
    "tweets_all.select(F.explode(F.col(\"entities.hashtags\"))).groupBy(\"col\").count().sort(\"count\", ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 2: Textual Analysis (LDA)\n",
    "Using the LDA algorithm provided by the Spark Machine Learning (ML) library, find out the ten most important topics. Use `s3://us-congress-tweets/trimmed/*.parquet` for this task (you can reuse `tweets_all` dataframe from Part1b). \n",
    "\n",
    "You may want to work on a small sample first but report your results on the whole dataset.\n",
    "\n",
    "Hint: for better results aggregate all tweets for a user into a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing\n",
      "Group by user IDs\n",
      "Aggregate posts by same user\n",
      "Split string into arrays\n",
      "Remove generic stopwords\n",
      "Explode list back into words\n",
      "Now remove all symbols and the word RT\n",
      "Preprocessing almost done... combining rows back into list\n",
      "CountVectorizer conversion\n",
      "LDA fitting\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices                                     |termWeights                                                                                                                                                                                                                    |\n",
      "+-----+------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[11, 15, 45, 274, 426, 69, 573, 242, 797, 906]  |[0.06198803078118324, 0.03794680047435259, 0.027187653966060647, 0.025140921186361055, 0.023314018711336166, 0.020144454114274464, 0.0156019830482875, 0.01542135384811979, 0.014006882521529228, 0.012784843499352397]        |\n",
      "|1    |[42, 11, 25, 60, 45, 12, 33, 73, 119, 76]       |[0.020480431338470537, 0.017138711448140987, 0.016054252602194428, 0.013577970306727466, 0.010357505212776704, 0.009553285655560378, 0.008890165356185021, 0.008247116850783647, 0.007234824019749669, 0.006782367355990843]   |\n",
      "|2    |[7, 2, 6, 1, 0, 3, 9, 22, 4, 10]                |[0.008938490684571306, 0.005942206329628726, 0.005923480248669214, 0.005343327880351338, 0.005153689025073599, 0.004684248912077625, 0.004365222303333265, 0.004304382532008659, 0.0040867221510271475, 0.004079285289886685]  |\n",
      "|3    |[0, 1, 2, 6, 4, 16, 3, 10, 9, 18]               |[0.013862540045111097, 0.008436230558417175, 0.005624019679401945, 0.005545537108142227, 0.005239030346473699, 0.004762721530564508, 0.004158489161774878, 0.004097323930034698, 0.0039858561544954514, 0.003888009309340535]  |\n",
      "|4    |[3, 0, 2, 5, 8, 4, 21, 1, 13, 12]               |[0.009565624667631542, 0.008547222116163315, 0.007507435319268555, 0.007008177193985768, 0.006163311060132866, 0.005792145455601668, 0.004700739564360839, 0.004388675673208637, 0.004335295428972649, 0.004328595958839514]   |\n",
      "|5    |[15, 48, 2, 12, 3, 5, 9, 4, 28, 42]             |[0.022971159554566332, 0.008537268417798683, 0.0069339168914208035, 0.00577461479820094, 0.005071909264631379, 0.0049684881393456054, 0.004822276193298815, 0.004343538424040703, 0.003911194430544108, 0.0031859143829668646] |\n",
      "|6    |[11, 45, 73, 60, 69, 119, 201, 293, 342, 393]   |[0.05806512379105358, 0.03227884307380554, 0.02984438732504212, 0.02926873679077207, 0.026790378457983953, 0.022620551055289713, 0.016547047198303137, 0.01243244468205365, 0.010975635488851936, 0.009874220771799183]        |\n",
      "|7    |[15, 48, 28, 215, 2, 1456, 42, 2688, 2705, 1163]|[0.05751250658254736, 0.019257284020953665, 0.005149188603626511, 0.00359983285191102, 0.003414237397711247, 0.0033590980098164827, 0.0032303606202118067, 0.0028764284740500517, 0.0028526957747493946, 0.0027189243108338926]|\n",
      "|8    |[10, 51, 15, 257, 298, 202, 48, 609, 1243, 198] |[0.010772158740018366, 0.010328385596004087, 0.009112392841847977, 0.008519565866854152, 0.007560153046405894, 0.007061428239486937, 0.0067078466335444495, 0.005698448549695781, 0.0055577312426947, 0.0054081578962116225]   |\n",
      "|9    |[1, 7, 0, 22, 2, 14, 5, 10, 3, 4]               |[0.010462073565433912, 0.010106016090152313, 0.007986039221230392, 0.007177852624894441, 0.006180133941148401, 0.005429572537344062, 0.005270700768994965, 0.004799955102678031, 0.004493607560692477, 0.004371824155003458]   |\n",
      "+-----+------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer, StopWordsRemover\n",
    "\n",
    "print('Start preprocessing')\n",
    "print('Group by user IDs')\n",
    "documents = tweets_all.select(F.col(\"user.id\"), \"text\").groupBy(\"id\").agg(F.collect_list(\"text\").alias(\"text\"))\n",
    "print('Aggregate posts by same user')\n",
    "documents = documents.withColumn(\"text\", F.concat_ws(\" \", \"text\")).sort(\"id\")\n",
    "print('Split string into arrays')\n",
    "documents = documents.select(\"id\", F.split(\"text\", \" \").alias(\"words\"))\n",
    "print('Remove generic stopwords')\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "documents = remover.transform(documents).select(\"id\", \"filtered\")\n",
    "print('Explode list back into words')\n",
    "documents = documents.select(\"id\", F.explode(\"filtered\"))\n",
    "documents = documents.select(\"id\", F.lower(F.col(\"col\")))\n",
    "print('Now remove all symbols and the word RT')\n",
    "expr = \"^[a-z]+$\"\n",
    "documents = documents.filter(documents[\"lower(col)\"].rlike(expr))\n",
    "documents = documents.filter(documents[\"lower(col)\"] != \"rt\")\n",
    "print('Preprocessing almost done... combining rows back into list')\n",
    "documents = documents.select(\"id\", \"lower(col)\").groupBy(\"id\").agg(F.collect_list(\"lower(col)\").alias(\"words\"))\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "print('CountVectorizer conversion')\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=65535, minDF=2.0)\n",
    "model = cv.fit(documents)\n",
    "result = model.transform(documents)\n",
    "\n",
    "# user1: aggie aggie -> [2, 0]\n",
    "# user2: aggie dior -> [1, 1]\n",
    "\n",
    "# Trains a LDA model.\n",
    "print('LDA fitting')\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model2 = lda.fit(result)\n",
    "\n",
    "topics = model2.describeTopics(10)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For each topic, print out 10 words to describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------+\n",
      "|topic|topic_desc                                                                       |\n",
      "+-----+---------------------------------------------------------------------------------+\n",
      "|0    |[de, transport, la, le, les, en, et, un, des, pour]                              |\n",
      "|1    |[please, de, thank, el, la, need, support, que, y, help]                         |\n",
      "|2    |[democrats, people, house, president, trump, like, us, border, one, american]    |\n",
      "|3    |[trump, president, people, house, one, senate, like, american, us, congress]     |\n",
      "|4    |[like, trump, people, get, know, one, think, president, time, need]              |\n",
      "|5    |[transport, public, people, need, like, get, us, one, new, please]               |\n",
      "|6    |[de, la, que, el, en, y, los, es, del, por]                                      |\n",
      "|7    |[transport, public, new, free, people, hong, please, tak, naik, road]            |\n",
      "|8    |[american, let, transport, men, become, little, public, poor, concentration, old]|\n",
      "|9    |[president, democrats, trump, border, people, want, get, american, like, one]    |\n",
      "+-----+---------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from pyspark import SparkContext\n",
    "vocab = model.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = F.udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "\n",
    "topics_mapped = topics.withColumn(\"topic_desc\", udf_map_termID_to_Word(topics.termIndices))\n",
    "topics_mapped.select(\"topic\", \"topic_desc\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 3a: MapRedce\n",
    "In this task, design a MapReduce program in python that reads all the original tweets (no retweets) in the sample tweets (`congress-sample-10k.json.gz`) and if a tweet is a reply to another tweet then output a record of the form <src_id, src_user, dst_id, dst_user>.\n",
    "\n",
    "Create a small cluster (2 or 3 nodes) as per the AWS Guide and then ssh to your cluster and use Hadoop streaming to execute your mapreduce program.\n",
    "\n",
    "Note: the Hadoop streaming jar file can be found at `/usr/lib/hadoop-mapreduce/hadoop-streaming.jar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your mapper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your reducer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Hadoop job submission command (copy/paste your command from the terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reply relationships did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to read job output and count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5 points) Part 3b: Going Large-Scale with MapReduce\n",
    "\n",
    "Rerun the same MapReduce job above but on the whole dataset (`s3://us-congress-tweets/raw/*.snappy`).\n",
    "All the files under `s3://us-congress-tweets/raw` can be read from the following file:\n",
    "\n",
    "`s3://us-congress-tweets/raw/files.txt`\n",
    "\n",
    "Use shell scripting to parse this file and prepare the input to your MapReduce job as  comma seperated string of all the files. (e.g. your input should be like this `s3://us-congress-tweets/raw/part-00000.snappy,s3://us-congress-tweets/raw/part-00001.snappy,s3://us-congress-tweets/raw/part-00002.snappy,...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the job logs, how many files did the job operate on? how many input splits were there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reply relationships did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of reply records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30 points) Part 4: Graph Analysis\n",
    "In this task, we would like to compute the longest path in *tweet reply* graphs and then perform some statistical calculations on the result. We will use Pregel implementation from GraphFrames for this task. Ignore paths that are longer than 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, construct your tweet reply network using tweet-reply records in this file `s3://us-congress-tweets/reply_network.csv`.\n",
    "From this file, use src_id and dst_id. The dst_id is the id of the tweet being replied to and the src_id is the id of the replying tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your network construction code here\n",
    "from graphframes import *\n",
    "from graphframes.lib import Pregel\n",
    "sc.setCheckpointDir(\"hdfs:///tmp/graphframes_checkpoint\") # this is needed for any GraphFrames operation\n",
    "reply_network = spark.read.csv(\"s3://us-congress-tweets/reply_network.csv\", header=True)\n",
    "reply_network = reply_network.sort(['src_id', 'dst_id'], ascending=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "vertices = reply_network.select('src_id')\n",
    "vertices.union(reply_network.select('dst_id'))\n",
    "vertices = vertices.distinct()\n",
    "vertices = vertices.selectExpr('src_id as id')\n",
    "edges = reply_network.select('src_id','dst_id')\n",
    "edges = edges.selectExpr('src_id as src','dst_id as dst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices, edges)\n",
    "indegrees = graph.inDegrees\n",
    "indegrees = indegrees.orderBy('inDegree', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top replied to tweets? (show 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|                 id|inDegree|\n",
      "+-------------------+--------+\n",
      "|1157787985041088513|   94351|\n",
      "|1048314564826292227|   76396|\n",
      "|1111289977143545856|   68172|\n",
      "|1155949756792725510|   65241|\n",
      "|1137060666223878144|   57764|\n",
      "|1062461047892787204|   53767|\n",
      "|1158036816089497601|   50059|\n",
      "|1144730911889428480|   45905|\n",
      "|1155949605147648006|   44810|\n",
      "|1098312693436596226|   41836|\n",
      "|1150408691713265665|   41825|\n",
      "|1129831615952236546|   41556|\n",
      "|1150859069084905472|   39020|\n",
      "|1144078421670150144|   38687|\n",
      "|1155132215208161281|   38572|\n",
      "|1088141172638400512|   37102|\n",
      "|1155469517092470784|   36961|\n",
      "|1168938037071482881|   36728|\n",
      "|1154161356171599877|   36552|\n",
      "|1131740851909083137|   36386|\n",
      "+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "indegrees.show(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many graphs in the reply network? (Hint: use connectedComponents function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|                 id|component|\n",
      "+-------------------+---------+\n",
      "|1047540049506533376|       26|\n",
      "|1047540679885373441|       29|\n",
      "|1047608661822959616|      474|\n",
      "|1047675670011109377|      964|\n",
      "|1047859519496249345|     1470|\n",
      "|1047862190311702528|     1697|\n",
      "|1047871839924621313|     1806|\n",
      "|1047881600938041344|     1950|\n",
      "|1047887394773585920|     2040|\n",
      "|1047899767890690048|     2214|\n",
      "|1047903874399461376|     2250|\n",
      "|1047922640390950913|     2453|\n",
      "|1047926495782608897|     2509|\n",
      "|1047927555611922434|     2529|\n",
      "|1047954452605546496|     2927|\n",
      "|1047965454638051328|     3091|\n",
      "|1048001380756144129|     3506|\n",
      "|1048025435345473536|     3764|\n",
      "|1048206673880518656|     4590|\n",
      "|1048223881528594432|     2533|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "connectedComponents = graph.connectedComponents()\n",
    "connectedComponents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT component)|\n",
      "+-------------------------+\n",
      "|                 51575731|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numComponents = connectedComponents.select(F.countDistinct(\"component\"))\n",
    "numComponents.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, design and execute a Pregel program that will calculate the longest paths for all reply graphs in the network. Explain your design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea is every node receives max(pregel.msg()) from \n",
    "# nodes that are directed to it.\n",
    "# A node with no incoming edge will always stay 0 as shown below,\n",
    "# using Coalesce\n",
    "# Another node that has only one incoming edge from the above node\n",
    "# will stay at 1 no matter how many iterations we run, as 0 is the only\n",
    "# msg it receives. \n",
    "# After n iterations, nodes that have the longest path of length \n",
    "# greater than n will have n. Other nodes that have those with smaller\n",
    "# length will converge to that length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your pregel code here\n",
    "import pyspark.sql.functions as F\n",
    "dists = graph.pregel \\\n",
    "             .setMaxIter(20) \\\n",
    "             .withVertexColumn(\"dist\", \n",
    "                               F.lit(0),\n",
    "                               F.coalesce(Pregel.msg() + F.lit(1), F.lit(0))\n",
    "                              ) \\\n",
    "             .sendMsgToDst(Pregel.src(\"dist\"))\\\n",
    "             .aggMsgs(F.max(Pregel.msg())).run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------------------+----+\n",
      "|                 id|component|                 id|dist|\n",
      "+-------------------+---------+-------------------+----+\n",
      "|1047537184780238848|        0|1047537184780238848|   2|\n",
      "|1047537374744498179|        1|1047537374744498179|   0|\n",
      "|1047537449914839040|        2|1047537449914839040|   0|\n",
      "|1047537740940828673|        3|1047537740940828673|   0|\n",
      "|1047537748826050561|        4|1047537748826050561|   0|\n",
      "|1047537817008705537|        5|1047537817008705537|   0|\n",
      "|1047538087675514880|        6|1047538087675514880|   0|\n",
      "|1047538120223334403|        7|1047538120223334403|   0|\n",
      "|1047538157472821248|        8|1047538157472821248|   0|\n",
      "|1047538405347872768|        9|1047538405347872768|   0|\n",
      "|1047538563015987200|       10|1047538563015987200|   3|\n",
      "|1047538681131753472|       11|1047538681131753472|   2|\n",
      "|1047538715516723202|       12|1047538715516723202|   0|\n",
      "|1047538733069885440|       13|1047538733069885440|   0|\n",
      "|1047538846605475844|       14|1047538846605475844|   0|\n",
      "|1047538864808763392|       15|1047538864808763392|   3|\n",
      "|1047538882827509760|       16|1047538882827509760|   1|\n",
      "|1047539063526494208|       17|1047539063526494208|   0|\n",
      "|1047539120325636096|       18|1047539120325636096|   7|\n",
      "|1047539182879612929|       19|1047539182879612929|   0|\n",
      "+-------------------+---------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "longest_paths = connectedComponents.join(dists, connectedComponents.id == dists.id)\n",
    "longest_paths.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average longest path length for all reply graphs in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "| component|max(dist)|\n",
      "+----------+---------+\n",
      "|8589934658|        0|\n",
      "|8589934965|        0|\n",
      "|8589935171|        0|\n",
      "|8589935183|        1|\n",
      "|8589935298|        0|\n",
      "|8589935317|        0|\n",
      "|8589935770|        1|\n",
      "|8589935936|        0|\n",
      "|8589936112|        0|\n",
      "|8589936348|        0|\n",
      "|8589936424|        0|\n",
      "|8589936566|        0|\n",
      "|8589936761|        9|\n",
      "|8589936870|        0|\n",
      "|8589936972|        1|\n",
      "|8589937582|       10|\n",
      "|8589937874|       20|\n",
      "|8589937892|        1|\n",
      "|8589937972|        5|\n",
      "|8589938024|        0|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "answer = longest_paths.select(\"component\", \"dist\").groupBy(\"component\").agg(F.max(\"dist\"))\n",
    "answer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(max(dist))=0.1638112506830005)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.select(\"max(dist)\").groupBy().avg().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My result may be lower by 1 than other results. If this is the case, this is because my algorithm computes\n",
    "# \"edge length\" of the longest paths. If you are looking for \"vertex length\" then the answer will be 1.1638..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30 points) Part 5: Community Detection\n",
    "User-hashtag relations have been extracted and saved in the file `s3://us-congress-tweets/user_hashtags.csv`. If a user uses a hashtag there will be a record with the userid and the hashtag.\n",
    "\n",
    "Use the Trawling algorithm discussed in class to find potential user communities in the dataset. (Hint: use FPGrowth in the Spark ML package). Explore different values for the support parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here. Explain all steps.\n",
    "import pyspark.sql.functions as F\n",
    "hash_tags = spark.read.csv(\"s3://us-congress-tweets/user_hashtags.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_tags_2 = hash_tags.groupby(\"shotoniphone\").agg(F.collect_list(\"154408627\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+\n",
      "|shotoniphone|collect_set(154408627)|\n",
      "+------------+----------------------+\n",
      "|       0by30|  [228852304, 27080...|\n",
      "| 100silences|  [825277375235633154]|\n",
      "| 1015PinCode|  [1116725511353294...|\n",
      "|       10ENE|  [150330846, 19434...|\n",
      "|    10points|          [1555824914]|\n",
      "|  12Russians|  [1059589752419168...|\n",
      "|  12Senators|  [42373491, 198080...|\n",
      "|     13Febre|  [78509222, 446281...|\n",
      "|    15thJTSR|  [1243575894, 5130...|\n",
      "|     1611KJB|  [704032426788425728]|\n",
      "|         17f|           [404845688]|\n",
      "|    187oNADA|  [9787211844718141...|\n",
      "|       1970s|  [107438367, 84199...|\n",
      "| 1986AMNESTY|            [15730096]|\n",
      "|      19DAYS|  [9392143817321512...|\n",
      "|      1Ahole|  [831606285086900225]|\n",
      "|  1AntifaDad|  [101896473, 24509...|\n",
      "|     1DDrive|  [8091302832913121...|\n",
      "|1DollarNancy|  [746565631391080450]|\n",
      "|          1L|  [1049838435928813...|\n",
      "+------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hash_tags_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_tags_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "fpGrowth = FPGrowth(itemsCol=\"collect_list(154408627)\", minSupport=0.0005)\n",
    "model = fpGrowth.fit(hash_tags_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o350.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 41.0 failed 4 times, most recent failure: Lost task 2.3 in stage 41.0 (TID 3345, ip-172-31-12-159.ec2.internal, executor 34): ExecutorLostFailure (executor 34 exited caused by one of the running tasks) Reason: Container from a bad node: container_1572567793815_0001_01_000039 on host: ip-172-31-12-159.ec2.internal. Exit status: 50. Diagnostics: Exception from container-launch.\nContainer id: container_1572567793815_0001_01_000039\nExit code: 50\nStack trace: ExitCodeException exitCode=50: \n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:972)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:869)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:235)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:299)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:83)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nContainer exited with a non-zero exit code 50\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6a14469ac93d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display frequent itemsets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCheckpointDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs:///tmp/graphframes_checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreqItemsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o350.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 41.0 failed 4 times, most recent failure: Lost task 2.3 in stage 41.0 (TID 3345, ip-172-31-12-159.ec2.internal, executor 34): ExecutorLostFailure (executor 34 exited caused by one of the running tasks) Reason: Container from a bad node: container_1572567793815_0001_01_000039 on host: ip-172-31-12-159.ec2.internal. Exit status: 50. Diagnostics: Exception from container-launch.\nContainer id: container_1572567793815_0001_01_000039\nExit code: 50\nStack trace: ExitCodeException exitCode=50: \n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:972)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:869)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:235)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:299)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:83)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nContainer exited with a non-zero exit code 50\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "# I encountered an unsolvable error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List two user communities you think are interesting. Explain why they are reasonable communities.\n",
    "\n",
    "You can use https://twitter.com/intent/user?user_id=? to find out more info about the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value for support did you choose and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 6: Personalized PageRank\n",
    "Assume you are given a task to recommend Twitter users for the speaker of the House to engage with.\n",
    "\n",
    "Construct a user-mentions network using relations in `s3://us-congress-tweets/user_mentions.csv`\n",
    "\n",
    "Run Personalized PageRank with source (id=15764644) and find out top accounts to recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your network construction code here\n",
    "\n",
    "from graphframes import *\n",
    "from graphframes.lib import Pregel\n",
    "sc.setCheckpointDir(\"hdfs:///tmp/graphframes_checkpoint\") # this is needed for any GraphFrames operation\n",
    "reply_network = spark.read.csv(\"s3://us-congress-tweets/user_mentions.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----+\n",
      "|     src|                dst|count|\n",
      "+--------+-------------------+-----+\n",
      "|15764644|          199325935|    5|\n",
      "|15764644|         1092979962|    4|\n",
      "|15764644|          381152398|    6|\n",
      "|15764644|1079854463211397120|    2|\n",
      "|15764644|         2916086925|    2|\n",
      "|15764644|          294084341|    1|\n",
      "|15764644| 716458790229581824|    1|\n",
      "|15764644|         1051127714|    2|\n",
      "|15764644| 855482971868069890|    3|\n",
      "|15764644|         2966570782|    2|\n",
      "|15764644|1076161611033968640|    2|\n",
      "|15764644|          122453931|    1|\n",
      "|15764644|           14857525|    1|\n",
      "|15764644|          233783568|    3|\n",
      "|15764644|1078355119920562176|    6|\n",
      "|15764644| 827258161841135623|    1|\n",
      "|15764644|         2244340904|    1|\n",
      "|15764644|         1339931490|    1|\n",
      "|15764644|           15751055|    1|\n",
      "|15764644|          584912320|    2|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reply_network = reply_network.filter(reply_network.src == 15764644)\n",
    "reply_network.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from graphframes.lib import Pregel\n",
    "sc.setCheckpointDir(\"hdfs:///tmp/graphframes_checkpoint\") # this is needed for any GraphFrames operation\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "vertices = reply_network.select('src')\n",
    "vertices.union(reply_network.select('dst'))\n",
    "vertices = vertices.selectExpr('src as id')\n",
    "vertices = vertices.distinct()\n",
    "edges = reply_network.select('src','dst')\n",
    "graph = GraphFrame(vertices, edges)\n",
    "vertices = graph.outDegrees\n",
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Personalized PageRank code here\n",
    "import pyspark.sql.functions as F # needed for defining literals below and the sum(...) function\n",
    "ranks = graph.pregel \\\n",
    "             .setMaxIter(5) \\\n",
    "             .withVertexColumn(\"rank\", \n",
    "                               F.lit(1.0),\n",
    "                               Pregel.msg() * F.lit(0.85) + F.lit(0.15) \n",
    "                              ) \\\n",
    "             .sendMsgToDst(Pregel.src(\"rank\")/Pregel.src(\"outDegree\"))\\\n",
    "             .aggMsgs(F.sum(Pregel.msg())).run()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+\n",
      "|      id|outDegree|               rank|\n",
      "+--------+---------+-------------------+\n",
      "|15764644|      342|0.15037373589338263|\n",
      "+--------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 accounts to recommend \n",
    "ranks.orderBy(\"rank\", ascending=False)\n",
    "ranks.show()\n",
    "\n",
    "# You can use https://twitter.com/intent/user?user_id=? to find out more info about the users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting Tips\n",
    "\n",
    "* If you get \"spark not available\" error, this most likely means the Kernel is python and not PySpark. Just change the Kernel to PySpark and it should work.\n",
    "\n",
    "\n",
    "* If your notebook seems stuck (may happen if you force stop a cell), you may need to ssh to your master node and kill the spark application associated with the notebook     \n",
    "    Use `yarn application -list` to find the application id and then `yarn application -kill app-id` to kill it. After that restart your notebook from the browser.\n",
    "\n",
    "\n",
    "* If you like, you may also ssh to the master node and run `pyspark` and execute your code directly in the shell.\n",
    "\n",
    "* If you face difficulties accessing the pages for the jobs for example to see logs and so on then you can open all needed ports when you create the cluster. (e.g. 8088)\n",
    "\n",
    "* If you want to see logs for a MapReduce job from the terminal use the following command:\n",
    "\n",
    "    `yarn logs -applicationId <application_id>`\n",
    "\n",
    "\n",
    "* To kill a MapReduce job use:\n",
    "\n",
    "    `yarn  application -kill <application_id>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
