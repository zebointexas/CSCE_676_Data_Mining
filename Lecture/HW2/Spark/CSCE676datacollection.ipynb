{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 676 :: Data Mining and Analysis :: Fall 2019\n",
    "\n",
    "\n",
    "# Data Collection\n",
    "\n",
    "*Notebook overview:* In this notebook, we're going to go over the basic of getting data through several handy methods:\n",
    "\n",
    "* Reading from a CSV file\n",
    "* Reading from a JSON file\n",
    "* Scraping from the web\n",
    "* Using an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part A. Reading from a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from a csv file as a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path='./data/births.csv'\n",
    "data = pd.read_csv(data_path, sep=',')\n",
    "data_sample = data[:5]\n",
    "print(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather than slicing the data, we can use head() to see the top few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the data type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can group data by different attributes\n",
    "grouped = data.groupby(['year', 'month', 'gender']).sum()\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data frame with female records\n",
    "data_f = data[data.gender=='F'].reset_index(drop=True)\n",
    "data_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data frame with number of births greater than 5500\n",
    "data_large_birth = data[data.births>5500].reset_index(drop=True)\n",
    "print(data_large_birth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new data frame with only two columns\n",
    "data_less_column = data[['year','gender']]\n",
    "print(data_less_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the those with birthday of Feb 28th or Feb 29th, the list type allow multiple selection.\n",
    "data_birth_selection = data[(data.day.isin(['29','28'])) & (data.month.isin([2]))].reset_index(drop=True)\n",
    "print(data_birth_selection[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from a csv file from the web\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/bob-ross/elements-by-episode.csv'\n",
    "\n",
    "bob_ross_data = pd.read_csv(url)\n",
    "bob_ross_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Reading from a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets try to read data in json format to python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_data_path='./data/tweets.json'\n",
    "with open(json_data_path,'r') as tweets_file:\n",
    "    for line in tweets_file:\n",
    "        line=json.loads(line)\n",
    "        print(line.keys())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: http://stackoverflow.com/questions/30088006/cant-figure-out-how-to-fix-the-error-in-the-following-code\n",
    "with open('./data/tweets.json', 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# remove the trailing \"\\n\" from each line\n",
    "data = map(lambda x: x.rstrip(), data)\n",
    "\n",
    "# each element of 'data' is an individual JSON object.\n",
    "# i want to convert it into an *array* of JSON objects\n",
    "# which, in and of itself, is one large JSON object\n",
    "# basically... add square brackets to the beginning\n",
    "# and end, and have all the individual business JSON objects\n",
    "# separated by a comma\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "# now, load it into pandas\n",
    "data_df = pd.read_json(data_json_str)\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More material on file I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Python Fundamentals Tutorial: Working with Files](https://newcircle.com/bookshelf/python_fundamentals_tutorial/working_with_files)\n",
    "* [Reading and Writing Files in Python](http://www.pythonforbeginners.com/files/reading-and-writing-files-in-python)\n",
    "* [JSON encoding and decoding with Python](https://pythonspot.com/en/json-encoding-and-decoding-with-python/)\n",
    "* [Text file (including csv) handling](http://nbviewer.jupyter.org/github/pydata/pydata-book/blob/master/ch06.ipynb)\n",
    "* [Getting started with pandas 1](http://nbviewer.jupyter.org/github/pydata/pydata-book/blob/master/ch05.ipynb)\n",
    "* [Getting started with pandas 2](http://nbviewer.jupyter.org/github/pydata/pydata-book/blob/master/ch06.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C. Scraping from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to crawl a webpage and extract the text in all h2 headers. This is done through using requests and beautifulsoup for html parsing.\n",
    "\n",
    "More info on Beautiful Soup:\n",
    "* [Web Scraping with Beautiful Soup](http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html)\n",
    "* [Intro to Beautiful Soup](http://programminghistorian.org/lessons/intro-to-beautiful-soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup,SoupStrainer\n",
    "import requests\n",
    "url='http://www.caverlee.com'\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in soup.find_all(\"h1\"):\n",
    "    print(a.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another package for website crawl is called **scrapy**. For parsing aditional packages widely used includes **lxml, xpath** and **HTMLParser**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D. Using an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how to crawl twitter users' timeline. Before access the api, there is an authentication process. OAuth is an authentication protocol that allows users to approve application to act on their behalf without sharing their password.Twitterâ€™s implementation is based on the Client Credentials Grant flow of the OAuth 2 specification. Thus you need to register your application at [link](https://apps.twitter.com/) in order to get the credentials. (consumer key, consumer token, access key, access token) The user timeline api is at [link](https://api.twitter.com/1.1/statuses/user_timeline.json?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import json\n",
    "\"\"\"Fill in the blanks here for your own Twitter app.\"\"\"\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\"\n",
    "consumer = oauth.Consumer(consumer_key, consumer_secret)\n",
    "token = oauth.Token(key=access_key, secret=access_secret)\n",
    "userlist = ['ev','CSE_at_TAMU']\n",
    "addr = 'https://api.twitter.com/1.1/statuses/user_timeline.json?screen_name=%s'\n",
    "client = oauth.Client(consumer, token)\n",
    "for uid in userlist:\n",
    "    resp, content = client.request(\n",
    "        addr%uid,\n",
    "        method='GET',\n",
    "        )\n",
    "    print(json.loads(content)[1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are many other librarys we can make use of. For Twitter, a nice wrapper named **tweepy** is readily available. \n",
    "\n",
    "To install: \n",
    "\n",
    "**pip install tweepy**\n",
    "\n",
    "Here is a sample crawler which has 3 member functions for crawling user profile and user tweets as well as rate limit checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import sys\n",
    "\n",
    "class TwitterCrawler():\n",
    "    '''Fill in the blanks here for your own Twitter app.'''\n",
    "    consumer_key = \"\"\n",
    "    consumer_secret = \"\"\n",
    "    access_key = \"\"\n",
    "    access_secret = \"\"\n",
    "    auth = None\n",
    "    api = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "        self.auth.set_access_token(self.access_key, self.access_secret)\n",
    "        self.api = tweepy.API(self.auth, parser=tweepy.parsers.JSONParser())\n",
    "        #print self.api.rate_limit_status()\n",
    "\n",
    "    def check_api_rate_limit(self, sleep_time):\n",
    "        try:\n",
    "            rate_limit_status = self.api.rate_limit_status()\n",
    "            print('------------check rate limit------------')\n",
    "            #print rate_limit_status\n",
    "        except Exception as error_message:\n",
    "            print(error_message)\n",
    "            if error_message['code'] == 88:\n",
    "                print(\"Sleeping for %d seconds.\" %(sleep_time))\n",
    "                print(rate_limit_status['resources']['statuses'])\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "    def crawl_user_profile(self, user_id):\n",
    "        try:\n",
    "            user_profile = self.api.get_user(user_id)\n",
    "        except:\n",
    "            return None\n",
    "        return user_profile\n",
    "\n",
    "    def crawl_user_tweets(self, user_id, count):\n",
    "        self.check_api_rate_limit(900)\n",
    "        page_cnt = 0\n",
    "        tried_count = 0\n",
    "        tweets= []\n",
    "        tweets_api_call=[]\n",
    "        while tweets_api_call!= None and len(tweets) < count:\n",
    "            try:\n",
    "                page_cnt += 1\n",
    "                tweets_api_call = self.api.user_timeline(user_id, count=count, page=page_cnt)\n",
    "                tweets.extend(tweets_api_call)\n",
    "            except:\n",
    "                pass\n",
    "            tried_count += 1\n",
    "            if tried_count == 5:\n",
    "                break\n",
    "        return tweets\n",
    "def main():\n",
    "    tc = TwitterCrawler()\n",
    "    user = tc.crawl_user_profile('TheRealCaverlee')\n",
    "    print(user)\n",
    "    tweets = tc.crawl_user_tweets('TheRealCaverlee', 500)\n",
    "    print(len(tweets))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### More on crawler\n",
    "* [Example twitter crawler](http://www.benkhalifa.com/twitter-crawler-python)\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
